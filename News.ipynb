{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a052d833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from scipy.stats import poisson\n",
    "\n",
    "class Environment: \n",
    "    \n",
    "    def __init__(self, distributions, profit, cost, numWeeks, maxDemand):\n",
    "        \n",
    "        if not len(distributions) == 7:\n",
    "            raise (\"there must be 7 distributions\")\n",
    "\n",
    "        self.distributions = distributions\n",
    "        self.day = 0\n",
    "        self.unit_profit = profit\n",
    "        self.unit_cost = cost \n",
    "        self.numWeeks = numWeeks\n",
    "        self.maxDemand = maxDemand\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Perform one internal step of the environment, given the agent has chosen ACTION\n",
    "        \"\"\"\n",
    "        # Sample from a demand distribution, make sure both the demand and action\n",
    "        # are integers within the acceptable range\n",
    "        demand = self.distributions[self.day % 7].rvs()\n",
    "        action = int(action)\n",
    "        demand = int(demand)\n",
    "\n",
    "        if action > self.maxDemand:\n",
    "            action = self.maxDemand\n",
    "        \n",
    "        if action < 0:\n",
    "            action = 0\n",
    "\n",
    "        if demand > self.maxDemand:\n",
    "            demand = self.maxDemand\n",
    "\n",
    "        if demand < 0:\n",
    "            demand = 0\n",
    "\n",
    "        # Calculate the net profit the agent generated for this timestep\n",
    "        sold = min(demand, action)\n",
    "        unsold = action - sold\n",
    "        profit = sold*self.unit_profit - unsold*self.unit_cost\n",
    "        \n",
    "        done = (self.day // 7 >= self.numWeeks)\n",
    "        self.day += 1\n",
    "\n",
    "        return profit, self.observe(), done\n",
    "\n",
    "    def reset(self):\n",
    "        self.day = 0\n",
    "\n",
    "    def observe(self):\n",
    "        return self.day%7\n",
    "\n",
    "    def getMaxDemand(self):\n",
    "        return self.maxDemand\n",
    "\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv('salesforcourse-4fe2kehu.csv')\n",
    "\n",
    "# 将日期列转换为星期几\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
    "\n",
    "# 根据星期几对数据进行分组，计算每个星期几的需求分布\n",
    "demand_distributions = []\n",
    "for day in range(7):\n",
    "    demand_distributions.append(poisson(df.loc[df['DayOfWeek'] == day]['Quantity'].mean()))\n",
    "\n",
    "# 实例化Environment类\n",
    "env = Environment(demand_distributions, df['Unit_Profit'].mean(), df['Unit Cost'].mean(), 10, df['Quantity'].max())\n",
    "from math import tau\n",
    "import torch\n",
    "from torch._C import device\n",
    "from torch.functional import norm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "from environment import Environment\n",
    "from utils import Transition, ReplayMemory, DQN, Distribution, ActorNet, CriticNet\n",
    "import scipy.stats as d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "class NewsvendorDDPGAgent(object):\n",
    "    \"\"\"\n",
    "    A class representing an AI agent that is able to interact with the environment, train its networks, \n",
    "    plot its progress and run for a number of episodes. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, replay_memory, action_range, gamma=0.99, experiment_name='default', \n",
    "            buffer_priming_period=1500, tau=0.005, mini_batch_size=128, noise_std_ratio=20, eval_t=2000):\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.env = env\n",
    "        self.replay_memory = replay_memory\n",
    "        self.action_range = action_range\n",
    "        self.gamma = gamma\n",
    "        self.experiment_name = experiment_name\n",
    "        self.buffer_priming_period = buffer_priming_period\n",
    "        self.tau = tau\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.results = []\n",
    "        self.averages = [[],[],[],[],[],[],[]]\n",
    "        self.t = 0\n",
    "        self.noise_std = (action_range[1] - action_range[0])/noise_std_ratio\n",
    "        self.state_size = 7\n",
    "        self.eval_t = eval_t\n",
    "        self.evaluate = False\n",
    "\n",
    "        cuda = torch.cuda.is_available()\n",
    "        self.device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "        # Initializing the neural networks \n",
    "        self.critic_net = self.create_critic_network(is_train=True)\n",
    "        self.critic_net_target = self.create_critic_network(is_train=False)\n",
    "        self.actor_net = self.create_actor_network(is_train=True)\n",
    "        self.actor_net_target = self.create_actor_network(is_train=False)\n",
    "        self.critic_net.to(self.device)\n",
    "        self.critic_net_target.to(self.device)\n",
    "        self.actor_net.to(self.device)\n",
    "        self.actor_net_target.to(self.device)\n",
    "\n",
    "    def select_action(self, day):\n",
    "        \"\"\"\n",
    "        Select an action given an input from the environment. For the first period, the agent selects \n",
    "        an action completely randomly to generate experience. After that period, it begins training its \n",
    "        networks and starts selecting to maximize reward\n",
    "        \"\"\"\n",
    "        state = self.day_to_state(day)\n",
    "\n",
    "        if self.t < self.buffer_priming_period:\n",
    "            # Random action\n",
    "            action = random.uniform(self.action_range[0], self.action_range[1])\n",
    "            action = torch.tensor([[action]], dtype=torch.float, device=self.device)\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                # Action chosen by the actor net with some added noise for exploration\n",
    "                action = self.actor_net(state)\n",
    "                noise = np.random.normal(0, self.noise_std)\n",
    "                if not self.evaluate:\n",
    "                    action += noise\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def create_critic_network(self, is_train=None):\n",
    "        \"\"\"\n",
    "        Creates a neuralnetwork of fully connected layers, taking in the state as well as the action, and \n",
    "        outputing the Q value of the resulting state.\n",
    "        \"\"\"\n",
    "        return CriticNet(self.state_size+1, 1)\n",
    "\n",
    "    def create_actor_network(self, is_train=None):\n",
    "        \"\"\"\n",
    "        Creates a neural network of fully connected layers, outbuting a single tensor representing \n",
    "        the chosen action. Takes in the the minimum and maximum possible actions to evenly spread its \n",
    "        output over that range.\n",
    "        \"\"\"\n",
    "        return ActorNet(self.state_size, output_size=1, min_out=self.action_range[0], max_out=self.action_range[1])\n",
    "\n",
    "    def update_target_networks(self, tau):\n",
    "        \"\"\"\n",
    "        Performs a soft update of the target networks, by averaging the previous parameters with the \n",
    "        current parameters by a small factor TAU.\n",
    "        \"\"\"\n",
    "        for target_param, param in zip(self.critic_net_target.parameters(), self.critic_net.parameters()):\n",
    "            target_param.data.copy_(target_param.data * (1.0 - tau) + param.data*tau)\n",
    "\n",
    "        for target_param, param in zip(self.actor_net_target.parameters(), self.actor_net.parameters()):\n",
    "            target_param.data.copy_(target_param.data * (1.0 - tau) + param.data*tau)\n",
    "    \n",
    "    def generate_target_q_values(self, next_state_batch, reward_batch):\n",
    "        \"\"\"\n",
    "        Generate the target Q values from a batch of next states, and the observed rewards\n",
    "        \"\"\"\n",
    "        # Find the actions we would've taken from the next states. Add some noise in as part of the TD3 algorithm\n",
    "        next_action_batch = self.actor_net_target(next_state_batch) + np.random.normal(0, self.noise_std)\n",
    "        # Take the minimum of the 'twin' networks learned by the critic, as part of the TD3 algorithm\n",
    "        q1, q2 = self.critic_net_target(next_state_batch, next_action_batch)\n",
    "        next_state_action_values = torch.min(q1, q2)\n",
    "        # Use the Bellman equation to calculate the true targets \n",
    "        target_values = reward_batch.unsqueeze(1) + self.gamma * next_state_action_values\n",
    "\n",
    "        return target_values\n",
    "\n",
    "    def train_minibatch(self):\n",
    "        \"\"\"\n",
    "        Samples a batch and updates the parameters/networks of the agent according to the sampled batch.\n",
    "        This means we ...\n",
    "            1. Compute the targets\n",
    "            2. Update the Q-function/critic by one step of gradient descent\n",
    "            3. Update the policy/actor by one step of gradient ascent\n",
    "            4. Update the target networks through a soft update\n",
    "        \"\"\"\n",
    "        if len(self.replay_memory) < self.mini_batch_size:\n",
    "            # Don't yet have enough memory experience to train\n",
    "            return\n",
    "        \n",
    "        # Sample a batch from memory and convert to tensors and store on device \n",
    "        mini_batch = self.replay_memory.sample(self.mini_batch_size)\n",
    "        batch = Transition(*zip(*mini_batch))\n",
    "\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        next_state_batch = torch.cat(batch.next_state)\n",
    "\n",
    "        state_batch.to(self.device)\n",
    "        action_batch.to(self.device)\n",
    "        reward_batch.to(self.device)\n",
    "        next_state_batch.to(self.device)\n",
    "        action_batch = action_batch.squeeze(dim=1)\n",
    "\n",
    "        # Train critic network\n",
    "        target_q_values = self.generate_target_q_values(next_state_batch, reward_batch)\n",
    "        self.critic_net.optimizer.zero_grad()\n",
    "        q1, q2 = self.critic_net(state_batch, action_batch)\n",
    "        \n",
    "        # Get proper dimensions of the tensors\n",
    "        target_q_values = target_q_values.squeeze(dim=1)\n",
    "        q1 = q1.squeeze(dim=1)\n",
    "        q2 = q2.squeeze(dim=1)\n",
    "        \n",
    "        # Loss here is the sum of the losses of each 'twin' network \n",
    "        value_loss = self.critic_net.criterion(q1, target_q_values) + self.critic_net.criterion(q2, target_q_values)\n",
    "        value_loss.backward()\n",
    "        self.critic_net.optimizer.step()\n",
    "\n",
    "        if self.t % 2 == 0:\n",
    "            # Perform a 'delayed' training of the actor network, as part of TD3\n",
    "            self.actor_net.optimizer.zero_grad()\n",
    "            actions_taken = self.actor_net(state_batch)\n",
    "            # Loss here is the opposite of the value of the actions the actor net would've taken,  \n",
    "            policy_loss = -self.critic_net.q1(state_batch, actions_taken).mean()\n",
    "            policy_loss.backward()\n",
    "            self.actor_net.optimizer.step()\n",
    "            self.update_target_networks(self.tau)\n",
    "            del policy_loss\n",
    "\n",
    "        del action_batch\n",
    "        del reward_batch\n",
    "        del state_batch\n",
    "        del next_state_batch\n",
    "        del value_loss\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Plot the reward the agent has achieved in every episode up to the present \n",
    "        \"\"\"\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Profit')\n",
    "        plt.title(f'{self.experiment_name} results:')\n",
    "        plt.plot(self.results)\n",
    "        plt.pause(0.01)\n",
    "\n",
    "\n",
    "    def day_to_state(self, day):\n",
    "        \"\"\"\n",
    "        Convert an int value representing the day to a binary tensor vector for the networks to use\n",
    "        \"\"\"\n",
    "        lst = [0]*7\n",
    "        lst[day] = 1\n",
    "        return torch.tensor([lst], dtype=torch.float, device=self.device)\n",
    "\n",
    "    def run(self, num_episodes):\n",
    "        \"\"\"\n",
    "        Main loop where the agent ineracts with the environemnts, chooses actions and trains its networks. Runs for \n",
    "        NUM_EPISODES until stopping.\n",
    "        \"\"\"\n",
    "        for _ in range(num_episodes):\n",
    "            # At the start of every episode, reset the environment and reward counter.\n",
    "            self.env.reset()\n",
    "            state = self.env.observe()\n",
    "            total_reward = 0\n",
    "\n",
    "            while True:\n",
    "                self.t += 1\n",
    "                # Select an action based on the state, interact with the environment, and observe the results\n",
    "                action = self.select_action(state)\n",
    "                reward, next_state, done = env.step(action)\n",
    "                if self.t % 226 == 0:\n",
    "                    # Log for debugging \n",
    "                    print(\"t:\", self.t, \"state: \", state, \" action: \", action, \" reward: \", reward)\n",
    "                total_reward += reward\n",
    "                \n",
    "                # Some linear algebra to make the tensors in memory match what the networks expect \n",
    "                mem_state = self.day_to_state(state)\n",
    "                mem_action = action.unsqueeze(dim=0)\n",
    "                mem_next_state = self.day_to_state(next_state)\n",
    "                mem_reward = torch.tensor([(reward/100)**3], dtype=torch.float, device = self.device)\n",
    "                \n",
    "                # Adding transition to memory \n",
    "                memory.push(mem_state, mem_action, mem_next_state, mem_reward)\n",
    "                \n",
    "                if self.t == self.buffer_priming_period:\n",
    "                    print(\"BEGIN TRAINING\")\n",
    "\n",
    "                if self.t > self.buffer_priming_period and not self.evaluate:\n",
    "                    self.train_minibatch()\n",
    "\n",
    "                if self.t == self.eval_t:\n",
    "                    print(\"BEGIN EVALUATING\")\n",
    "                    self.evaluate = True\n",
    "\n",
    "                if self.evaluate:\n",
    "                    self.averages[state].append(action.data)\n",
    "                \n",
    "                # Advance to next state\n",
    "                state = next_state\n",
    "                \n",
    "                if done:\n",
    "                    self.results.append(total_reward/(self.env.numWeeks*7))\n",
    "                    self.plot()\n",
    "                    break\n",
    "import torch\n",
    "from torch.functional import norm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    # Set up memory storage\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    # Simple neural net consisting only of fully connected linear layers. Input is a 1x1 tensor and output is a 1x2 tensor representing the expected discounted value of choosing each action\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class Distribution(object):\n",
    "\n",
    "    def __init__(self, dist, mean, sd):\n",
    "        self.dist = dist\n",
    "        self.mean = mean\n",
    "        self.sd = sd\n",
    "    \n",
    "    def sample(self):\n",
    "        return self.dist.rvs(loc=self.mean, scale=self.sd)\n",
    "\n",
    "\n",
    "class CriticNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size=1):\n",
    "        super(CriticNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, output_size)\n",
    "\n",
    "        self.fc5 = nn.Linear(input_size, 64)\n",
    "        self.fc6 = nn.Linear(64, 32)\n",
    "        self.fc7 = nn.Linear(32, 16)\n",
    "        self.fc8 = nn.Linear(16, output_size)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters())\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, states, actions):\n",
    "        x = torch.cat((states, actions), 1)\n",
    "        q1 = F.relu(self.fc1(x))\n",
    "        q1 = F.relu(self.fc2(q1))\n",
    "        q1 = F.relu(self.fc3(q1))\n",
    "        q1 = self.fc4(q1)\n",
    "\n",
    "        q2 = F.relu(self.fc5(x))\n",
    "        q2 = F.relu(self.fc6(q2))\n",
    "        q2 = F.relu(self.fc7(q2))\n",
    "        q2 = self.fc8(q2)\n",
    "\n",
    "        return q1, q2\n",
    "\n",
    "    def q1(self, states, actions):\n",
    "        x = torch.cat((states, actions), 1)\n",
    "        q = F.relu(self.fc1(x))\n",
    "        q = F.relu(self.fc2(q))\n",
    "        q = F.relu(self.fc3(q))\n",
    "\n",
    "        return self.fc4(q)\n",
    "\n",
    "\n",
    "class ActorNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size=1, min_out=0, max_out=1 ):\n",
    "        super(ActorNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, output_size)\n",
    "        self.min_out = min_out\n",
    "        self.max_out = max_out\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.tanh(self.fc4(x))\n",
    "        \n",
    "        dev = (self.max_out - self.min_out)/2\n",
    "        mid = (self.max_out + self.min_out)/2\n",
    "        x = mid + dev*x        \n",
    "\n",
    "        return x\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
